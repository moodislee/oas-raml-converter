#%RAML 1.0
title: Google Cloud Vision
version: v1
baseUri: 'https://vision.googleapis.com/'
protocols:
  - HTTPS
description: 'Integrates Google Vision features, including image labeling, face, logo, and landmark detection, optical character recognition (OCR), and detection of explicit content, into applications.'
(oas-tags-definition):
  - name: images
annotationTypes:
  oas-tags-definition:
    type: array
    items:
      properties:
        name: string
        description?: string
        externalDocs?:
          properties:
            url: string
            description?: string
    allowedTargets: API
  oas-x-apiClientRegistration: any
  oas-x-logo: any
  oas-x-origin: any
  oas-x-preferred: any
  oas-x-providerName: any
  oas-x-serviceName: any
  oas-tags:
    type: 'string[]'
    allowedTargets: Method
  oas-externalDocs:
    properties:
      description?: string
      url: string
    allowedTargets:
      - API
      - Method
      - TypeDeclaration
  oas-info:
    properties:
      termsOfService?: string
      contact?:
        properties:
          name?: string
          url?: string
          email?: string
      license?:
        properties:
          name?: string
          url?: string
    allowedTargets: API
  oas-body-name:
    type: string
    allowedTargets: TypeDeclaration
(oas-info):
  (oas-x-apiClientRegistration):
    url: 'https://console.developers.google.com'
  (oas-x-logo):
    url: 'https://api.apis.guru/v2/cache/logo/https_www.google.com_images_branding_googlelogo_2x_googlelogo_color_272x92dp.png'
  (oas-x-origin):
    format: google
    url: 'https://vision.googleapis.com/$discovery/rest?version=v1'
    version: v1
  (oas-x-preferred): true
  (oas-x-providerName): googleapis.com
  (oas-x-serviceName): vision
  contact:
    name: Google
    url: 'https://google.com'
(oas-externalDocs):
  url: 'https://cloud.google.com/vision/'
securitySchemes:
  Oauth2:
    type: OAuth 2.0
    settings:
      authorizationUri: 'https://accounts.google.com/o/oauth2/auth'
      accessTokenUri: ''
      authorizationGrants:
        - implicit
      scopes:
        - 'https://www.googleapis.com/auth/cloud-platform'
    description: Oauth 2.0 authentication
/v1:
  displayName: v1
  '/images:annotate':
    displayName: 'images:annotate'
    post:
      displayName: vision.images.annotate
      description: Run image detection and annotation for a batch of images.
      body:
        application/json:
          type: BatchAnnotateImagesRequest
          (oas-body-name): body
      responses:
        '200':
          body:
            application/json:
              type: BatchAnnotateImagesResponse
          description: Successful response
      securedBy:
        - Oauth2:
            scopes:
              - 'https://www.googleapis.com/auth/cloud-platform'
      (oas-tags):
        - images
    is:
      - accessToken
      - prettyPrint
      - key
      - quotaUser
      - pp
      - fields
      - alt
      - xgafv
      - callback
      - oauthToken
      - uploadType
      - bearerToken
      - uploadProtocol
types:
  AnnotateImageRequest:
    description: |-
      Request for performing Google Cloud Vision API tasks over a user-provided
      image, with user-requested features.
    properties:
      features:
        description: Requested features.
        items:
          type: Feature
        type: array
        required: false
      image:
        description: The image to be processed.
        type: Image
        required: false
      imageContext:
        description: Additional context that may accompany the image.
        type: ImageContext
        required: false
  AnnotateImageResponse:
    description: Response to an image annotation request.
    properties:
      error:
        description: |-
          If set, represents the error message for the operation.
          Note that filled-in image annotations are guaranteed to be
          correct, even when <code>error</code> is non-empty.
        type: Status
        required: false
      faceAnnotations:
        description: 'If present, face detection completed successfully.'
        items:
          type: FaceAnnotation
        type: array
        required: false
      imagePropertiesAnnotation:
        description: 'If present, image properties were extracted successfully.'
        type: ImageProperties
        required: false
      labelAnnotations:
        description: 'If present, label detection completed successfully.'
        items:
          type: EntityAnnotation
        type: array
        required: false
      landmarkAnnotations:
        description: 'If present, landmark detection completed successfully.'
        items:
          type: EntityAnnotation
        type: array
        required: false
      logoAnnotations:
        description: 'If present, logo detection completed successfully.'
        items:
          type: EntityAnnotation
        type: array
        required: false
      safeSearchAnnotation:
        description: 'If present, safe-search annotation completed successfully.'
        type: SafeSearchAnnotation
        required: false
      textAnnotations:
        description: 'If present, text (OCR) detection completed successfully.'
        items:
          type: EntityAnnotation
        type: array
        required: false
  BatchAnnotateImagesRequest:
    description: Multiple image annotation requests are batched into a single service call.
    properties:
      requests:
        description: Individual image annotation requests for this batch.
        items:
          type: AnnotateImageRequest
        type: array
        required: false
  BatchAnnotateImagesResponse:
    description: Response to a batch image annotation request.
    properties:
      responses:
        description: Individual responses to image annotation requests within the batch.
        items:
          type: AnnotateImageResponse
        type: array
        required: false
  BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      vertices:
        description: The bounding polygon vertices.
        items:
          type: Vertex
        type: array
        required: false
  Color:
    description: |-
      Represents a color in the RGBA color space. This representation is designed
      for simplicity of conversion to/from color representations in various
      languages over compactness; for example, the fields of this representation
      can be trivially provided to the constructor of "java.awt.Color" in Java; it
      can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha"
      method in iOS; and, with just a little work, it can be easily formatted into
      a CSS "rgba()" string in JavaScript, as well. Here are some examples:

      Example (Java):

           import com.google.type.Color;

           // ...
           public static java.awt.Color fromProto(Color protocolor) {
             float alpha = protocolor.hasAlpha()
                 ? protocolor.getAlpha().getValue()
                 : 1.0;

             return new java.awt.Color(
                 protocolor.getRed(),
                 protocolor.getGreen(),
                 protocolor.getBlue(),
                 alpha);
           }

           public static Color toProto(java.awt.Color color) {
             float red = (float) color.getRed();
             float green = (float) color.getGreen();
             float blue = (float) color.getBlue();
             float denominator = 255.0;
             Color.Builder resultBuilder =
                 Color
                     .newBuilder()
                     .setRed(red / denominator)
                     .setGreen(green / denominator)
                     .setBlue(blue / denominator);
             int alpha = color.getAlpha();
             if (alpha != 255) {
               result.setAlpha(
                   FloatValue
                       .newBuilder()
                       .setValue(((float) alpha) / denominator)
                       .build());
             }
             return resultBuilder.build();
           }
           // ...

      Example (iOS / Obj-C):

           // ...
           static UIColor* fromProto(Color* protocolor) {
              float red = [protocolor red];
              float green = [protocolor green];
              float blue = [protocolor blue];
              FloatValue* alpha_wrapper = [protocolor alpha];
              float alpha = 1.0;
              if (alpha_wrapper != nil) {
                alpha = [alpha_wrapper value];
              }
              return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
           }

           static Color* toProto(UIColor* color) {
               CGFloat red, green, blue, alpha;
               if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) {
                 return nil;
               }
               Color* result = [Color alloc] init];
               [result setRed:red];
               [result setGreen:green];
               [result setBlue:blue];
               if (alpha <= 0.9999) {
                 [result setAlpha:floatWrapperWithValue(alpha)];
               }
               [result autorelease];
               return result;
          }
          // ...

       Example (JavaScript):

          // ...

          var protoToCssColor = function(rgb_color) {
             var redFrac = rgb_color.red || 0.0;
             var greenFrac = rgb_color.green || 0.0;
             var blueFrac = rgb_color.blue || 0.0;
             var red = Math.floor(redFrac * 255);
             var green = Math.floor(greenFrac * 255);
             var blue = Math.floor(blueFrac * 255);

             if (!('alpha' in rgb_color)) {
                return rgbToCssColor_(red, green, blue);
             }

             var alphaFrac = rgb_color.alpha.value || 0.0;
             var rgbParams = [red, green, blue].join(',');
             return ['rgba(', rgbParams, ',', alphaFrac, ')'].join('');
          };

          var rgbToCssColor_ = function(red, green, blue) {
            var rgbNumber = new Number((red << 16) | (green << 8) | blue);
            var hexString = rgbNumber.toString(16);
            var missingZeros = 6 - hexString.length;
            var resultBuilder = ['#'];
            for (var i = 0; i < missingZeros; i++) {
               resultBuilder.push('0');
            }
            resultBuilder.push(hexString);
            return resultBuilder.join('');
          };

          // ...
    properties:
      alpha:
        description: |-
          The fraction of this color that should be applied to the pixel. That is,
          the final pixel color is defined by the equation:

            pixel color = alpha * (this color) + (1.0 - alpha) * (background color)

          This means that a value of 1.0 corresponds to a solid color, whereas
          a value of 0.0 corresponds to a completely transparent color. This
          uses a wrapper message rather than a simple float scalar so that it is
          possible to distinguish between a default value and the value being unset.
          If omitted, this color object is to be rendered as a solid color
          (as if the alpha value had been explicitly given with a value of 1.0).
        format: float
        type: number
        required: false
      blue:
        description: 'The amount of blue in the color as a value in the interval [0, 1].'
        format: float
        type: number
        required: false
      green:
        description: 'The amount of green in the color as a value in the interval [0, 1].'
        format: float
        type: number
        required: false
      red:
        description: 'The amount of red in the color as a value in the interval [0, 1].'
        format: float
        type: number
        required: false
  ColorInfo:
    description: |-
      Color information consists of RGB channels, score and fraction of
      image the color occupies in the image.
    properties:
      color:
        description: RGB components of the color.
        type: Color
        required: false
      pixelFraction:
        description: |-
          Stores the fraction of pixels the color occupies in the image.
          Value in range [0, 1].
        format: float
        type: number
        required: false
      score:
        description: 'Image-specific score for this color. Value in range [0, 1].'
        format: float
        type: number
        required: false
  DominantColorsAnnotation:
    description: Set of dominant colors and their corresponding scores.
    properties:
      colors:
        description: 'RGB color values, with their score and pixel fraction.'
        items:
          type: ColorInfo
        type: array
        required: false
  EntityAnnotation:
    description: Set of detected entity features.
    properties:
      boundingPoly:
        description: |-
          Image region to which this entity belongs. Not filled currently
          for `LABEL_DETECTION` features. For `TEXT_DETECTION` (OCR), `boundingPoly`s
          are produced for the entire text detected in an image region, followed by
          `boundingPoly`s for each word within the detected text.
        type: BoundingPoly
        required: false
      confidence:
        description: |-
          The accuracy of the entity detection in an image.
          For example, for an image containing 'Eiffel Tower,' this field represents
          the confidence that there is a tower in the query image. Range [0, 1].
        format: float
        type: number
        required: false
      description:
        description: 'Entity textual description, expressed in its <code>locale</code> language.'
        type: string
        required: false
      locale:
        description: |-
          The language code for the locale in which the entity textual
          <code>description</code> (next field) is expressed.
        type: string
        required: false
      locations:
        description: |-
          The location information for the detected entity. Multiple
          <code>LocationInfo</code> elements can be present since one location may
          indicate the location of the scene in the query image, and another the
          location of the place where the query image was taken. Location information
          is usually present for landmarks.
        items:
          type: LocationInfo
        type: array
        required: false
      mid:
        description: |-
          Opaque entity ID. Some IDs might be available in Knowledge Graph(KG).
          For more details on KG please see:
          https://developers.google.com/knowledge-graph/
        type: string
        required: false
      properties:
        description: |-
          Some entities can have additional optional <code>Property</code> fields.
          For example a different kind of score or string that qualifies the entity.
        items:
          type: Property
        type: array
        required: false
      score:
        description: 'Overall score of the result. Range [0, 1].'
        format: float
        type: number
        required: false
      topicality:
        description: |-
          The relevancy of the ICA (Image Content Annotation) label to the
          image. For example, the relevancy of 'tower' to an image containing
          'Eiffel Tower' is likely higher than an image containing a distant towering
          building, though the confidence that there is a tower may be the same.
          Range [0, 1].
        format: float
        type: number
        required: false
  FaceAnnotation:
    description: A face annotation object contains the results of face detection.
    properties:
      angerLikelihood:
        description: Anger likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      blurredLikelihood:
        description: Blurred likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      boundingPoly:
        description: |-
          The bounding polygon around the face. The coordinates of the bounding box
          are in the original image's scale, as returned in ImageParams.
          The bounding box is computed to "frame" the face in accordance with human
          expectations. It is based on the landmarker results.
          Note that one or more x and/or y coordinates may not be generated in the
          BoundingPoly (the polygon will be unbounded) if only a partial face appears in
          the image to be annotated.
        type: BoundingPoly
        required: false
      detectionConfidence:
        description: 'Detection confidence. Range [0, 1].'
        format: float
        type: number
        required: false
      fdBoundingPoly:
        description: |-
          This bounding polygon is tighter than the previous
          <code>boundingPoly</code>, and
          encloses only the skin part of the face. Typically, it is used to
          eliminate the face from any image analysis that detects the
          "amount of skin" visible in an image. It is not based on the
          landmarker results, only on the initial face detection, hence
          the <code>fd</code> (face detection) prefix.
        type: BoundingPoly
        required: false
      headwearLikelihood:
        description: Headwear likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      joyLikelihood:
        description: Joy likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      landmarkingConfidence:
        description: 'Face landmarking confidence. Range [0, 1].'
        format: float
        type: number
        required: false
      landmarks:
        description: Detected face landmarks.
        items:
          type: Landmark
        type: array
        required: false
      panAngle:
        description: |-
          Yaw angle. Indicates the leftward/rightward angle that the face is
          pointing, relative to the vertical plane perpendicular to the image. Range
          [-180,180].
        format: float
        type: number
        required: false
      rollAngle:
        description: |-
          Roll angle. Indicates the amount of clockwise/anti-clockwise rotation of
          the
          face relative to the image vertical, about the axis perpendicular to the
          face. Range [-180,180].
        format: float
        type: number
        required: false
      sorrowLikelihood:
        description: Sorrow likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      surpriseLikelihood:
        description: Surprise likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      tiltAngle:
        description: |-
          Pitch angle. Indicates the upwards/downwards angle that the face is
          pointing
          relative to the image's horizontal plane. Range [-180,180].
        format: float
        type: number
        required: false
      underExposedLikelihood:
        description: Under-exposed likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
  Feature:
    description: |-
      The <em>Feature</em> indicates what type of image detection task to perform.
      Users describe the type of Google Cloud Vision API tasks to perform over
      images by using <em>Feature</em>s. Features encode the Cloud Vision API
      vertical to operate on and the number of top-scoring results to return.
    properties:
      maxResults:
        description: Maximum number of results of this type.
        format: int32
        type: integer
        required: false
      type:
        description: The feature type.
        enum:
          - TYPE_UNSPECIFIED
          - FACE_DETECTION
          - LANDMARK_DETECTION
          - LOGO_DETECTION
          - LABEL_DETECTION
          - TEXT_DETECTION
          - SAFE_SEARCH_DETECTION
          - IMAGE_PROPERTIES
        type: string
        required: false
  Image:
    description: Client image to perform Google Cloud Vision API tasks over.
    properties:
      content:
        description: |-
          Image content, represented as a stream of bytes.
          Note: as with all `bytes` fields, protobuffers use a pure binary
          representation, whereas JSON representations use base64.
        format: byte
        type: string
        facets:
          format: string
        required: false
      source:
        description: |-
          Google Cloud Storage image location. If both 'content' and 'source'
          are filled for an image, 'content' takes precedence and it will be
          used for performing the image annotation request.
        type: ImageSource
        required: false
  ImageContext:
    description: Image context and/or feature-specific parameters.
    properties:
      languageHints:
        description: |-
          List of languages to use for TEXT_DETECTION. In most cases, an empty value
          yields the best results since it enables automatic language detection. For
          languages based on the Latin alphabet, setting `language_hints` is not
          needed. In rare cases, when the language of the text in the image is known,
          setting a hint will help get better results (although it will be a
          significant hindrance if the hint is wrong). Text detection returns an
          error if one or more of the specified languages is not one of the
          [supported
          languages](/vision/docs/languages).
        items:
          type: string
        type: array
        required: false
      latLongRect:
        description: Lat/long rectangle that specifies the location of the image.
        type: LatLongRect
        required: false
  ImageProperties:
    description: Stores image properties (e.g. dominant colors).
    properties:
      dominantColors:
        description: 'If present, dominant colors completed successfully.'
        type: DominantColorsAnnotation
        required: false
  ImageSource:
    description: External image source (Google Cloud Storage image location).
    properties:
      gcsImageUri:
        description: |-
          Google Cloud Storage image URI. It must be in the following form:
          `gs://bucket_name/object_name`. For more
          details, please see: https://cloud.google.com/storage/docs/reference-uris.
          NOTE: Cloud Storage object versioning is not supported!
        type: string
        required: false
  Landmark:
    description: |-
      A face-specific landmark (for example, a face feature).
      Landmark positions may fall outside the bounds of the image
      when the face is near one or more edges of the image.
      Therefore it is NOT guaranteed that 0 <= x < width or 0 <= y < height.
    properties:
      position:
        description: Face landmark position.
        type: Position
        required: false
      type:
        description: Face landmark type.
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
        type: string
        required: false
  LatLng:
    description: |-
      An object representing a latitude/longitude pair. This is expressed as a pair
      of doubles representing degrees latitude and degrees longitude. Unless
      specified otherwise, this must conform to the
      <a href="http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf">WGS84
      standard</a>. Values must be within normalized ranges.

      Example of normalization code in Python:

          def NormalizeLongitude(longitude):
            """Wraps decimal degrees longitude to [-180.0, 180.0]."""
            q, r = divmod(longitude, 360.0)
            if r > 180.0 or (r == 180.0 and q <= -1.0):
              return r - 360.0
            return r

          def NormalizeLatLng(latitude, longitude):
            """Wraps decimal degrees latitude and longitude to
            [-90.0, 90.0] and [-180.0, 180.0], respectively."""
            r = latitude % 360.0
            if r <= 90.0:
              return r, NormalizeLongitude(longitude)
            elif r >= 270.0:
              return r - 360, NormalizeLongitude(longitude)
            else:
              return 180 - r, NormalizeLongitude(longitude + 180.0)

          assert 180.0 == NormalizeLongitude(180.0)
          assert -180.0 == NormalizeLongitude(-180.0)
          assert -179.0 == NormalizeLongitude(181.0)
          assert (0.0, 0.0) == NormalizeLatLng(360.0, 0.0)
          assert (0.0, 0.0) == NormalizeLatLng(-360.0, 0.0)
          assert (85.0, 180.0) == NormalizeLatLng(95.0, 0.0)
          assert (-85.0, -170.0) == NormalizeLatLng(-95.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(90.0, 10.0)
          assert (-90.0, -10.0) == NormalizeLatLng(-90.0, -10.0)
          assert (0.0, -170.0) == NormalizeLatLng(-180.0, 10.0)
          assert (0.0, -170.0) == NormalizeLatLng(180.0, 10.0)
          assert (-90.0, 10.0) == NormalizeLatLng(270.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(-270.0, 10.0)
    properties:
      latitude:
        description: 'The latitude in degrees. It must be in the range [-90.0, +90.0].'
        format: double
        type: number
        required: false
      longitude:
        description: 'The longitude in degrees. It must be in the range [-180.0, +180.0].'
        format: double
        type: number
        required: false
  LatLongRect:
    description: Rectangle determined by min and max LatLng pairs.
    properties:
      maxLatLng:
        description: Max lat/long pair.
        type: LatLng
        required: false
      minLatLng:
        description: Min lat/long pair.
        type: LatLng
        required: false
  LocationInfo:
    description: Detected entity location information.
    properties:
      latLng:
        description: Lat - long location coordinates.
        type: LatLng
        required: false
  Position:
    description: |-
      A 3D position in the image, used primarily for Face detection landmarks.
      A valid Position must have both x and y coordinates.
      The position coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
        required: false
      'y':
        description: Y coordinate.
        format: float
        type: number
        required: false
      z:
        description: Z coordinate (or depth).
        format: float
        type: number
        required: false
  Property:
    description: Arbitrary name/value pair.
    properties:
      name:
        description: Name of the property.
        type: string
        required: false
      value:
        description: Value of the property.
        type: string
        required: false
  SafeSearchAnnotation:
    description: |-
      Set of features pertaining to the image, computed by various computer vision
      methods over safe-search verticals (for example, adult, spoof, medical,
      violence).
    properties:
      adult:
        description: Represents the adult contents likelihood for the image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      medical:
        description: Likelihood this is a medical image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      spoof:
        description: |-
          Spoof likelihood. The likelihood that an obvious modification
          was made to the image's canonical version to make it appear
          funny or offensive.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
      violence:
        description: Violence likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
        required: false
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for different
      programming environments, including REST APIs and RPC APIs. It is used by
      [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error message,
      and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` which can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting purpose.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
        required: false
      details:
        description: |-
          A list of messages that carry the error details.  There will be a
          common set of message types for APIs to use.
        items:
          type: object
          properties:
            //:
              description: Properties of the object. Contains field @type with type URL.
        type: array
        required: false
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
        required: false
  Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
        required: false
      'y':
        description: Y coordinate.
        format: int32
        type: integer
        required: false
traits:
  xgafv:
    queryParameters:
      $.xgafv:
        type: string
        description: V1 error format.
        enum:
          - '1'
          - '2'
  accessToken:
    queryParameters:
      access_token:
        type: string
        description: OAuth access token.
  alt:
    queryParameters:
      alt:
        type: string
        description: Data format for response.
        default: json
        enum:
          - json
          - media
          - proto
  bearerToken:
    queryParameters:
      bearer_token:
        type: string
        description: OAuth bearer token.
  callback:
    queryParameters:
      callback:
        type: string
        description: JSONP
  fields:
    queryParameters:
      fields:
        type: string
        description: Selector specifying which fields to include in a partial response.
  key:
    queryParameters:
      key:
        type: string
        description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
  oauthToken:
    queryParameters:
      oauth_token:
        type: string
        description: OAuth 2.0 token for the current user.
  pp:
    queryParameters:
      pp:
        type: boolean
        description: Pretty-print response.
        default: true
  prettyPrint:
    queryParameters:
      prettyPrint:
        type: boolean
        description: Returns response with indentations and line breaks.
        default: true
  quotaUser:
    queryParameters:
      quotaUser:
        type: string
        description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
  uploadType:
    queryParameters:
      uploadType:
        type: string
        description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
  uploadProtocol:
    queryParameters:
      upload_protocol:
        type: string
        description: 'Upload protocol for media (e.g. "raw", "multipart").'
